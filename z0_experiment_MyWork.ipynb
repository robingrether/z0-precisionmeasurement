{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Optimize lepton selection\n",
    "\n",
    "* First, print the distributions of the relevant variables for *all* the Monte Carlo samples (i.e. all the *channels* of the $Z$-boson decay to be studied). Which variables are these? Give sensible ranges to include all the events in the samples (both MC and OPAL data) \n",
    "* Do the same for **one** of the OPAL data samples (your lab assistant will decide which one you choose).\n",
    "* Describe the results.\n",
    "* Optimize the object selection by applying cuts. Make a strategy on how to proceed to find the optimal selection. which information do you need? in thin the\n",
    "* Determine the efficiency and the amount of background for each $Z$ decay channel. Use the simulated events $e^+e^-$, $\\mu^+\\mu^-$, $\\tau^+\\tau^-$ and hadrons ($qq$). Represent the result in a matrix form and think carefully about how you have to correct the measured rates. Don't forget to calculate the errors!\n",
    "* How do we estimate the statistical fluctuations per bin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and import libraries\n",
    "Comment in the following two lines in case some of the libraries cannot be imported. Please restart the kernel after download+upgrade has successfully finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download libraries\n",
    "#%pip install uproot\n",
    "#%pip install awkward\n",
    "#%pip install mplhep\n",
    "#%pip install numpy\n",
    "#%pip install matplotlib\n",
    "#%pip install scipy\n",
    "\n",
    "### Upgrade libraries to latest version\n",
    "#%pip install uproot awkward mplhep numpy matplotlib scipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import mplhep\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 11})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Monte Carlo Samples and define Cuts\n",
    "First, we will open the Monte Carlo samples using `uproot`.\n",
    "\n",
    "The datasets are converted to a pandas data frame. Available keys are:\n",
    "\n",
    "| Variable name | Description |\n",
    "| --- | --- | \n",
    "| ``run`` | Run number |\n",
    "| ``event`` | Event number |\n",
    "| ``Ncharged`` | Number of charged tracks |\n",
    "| ``Pcharged`` | Total scalar sum of track momenta |\n",
    "| ``E_ecal`` | Total energy measured in the electromagnetic calorimeter |\n",
    "| ``E_hcal`` | Total energy measured in the hadronic calorimete |\n",
    "| ``E_lep`` | LEP beam energy (=$\\sqrt{s}/2$) |\n",
    "| ``cos_thru`` | cosine of the polar angle between beam axis and thrust axis |\n",
    "| ``cos_thet`` | cosine of the polar angle between incoming positron and outgoing positive particle |\n",
    "\n",
    "If ``cos_thru`` or ``cos_thet`` are undefined, the value ``999`` is used to idicate that. This is replaced by ``NaN`` to ensure correct behaviour in calculations.\n",
    "\n",
    "Furthermore, we reject values larger $100\\,\\mathrm{MeV}$ for ``Pcharged`` as unphysical, replacing them also with ``NaN``. This is necessary, since outliers $>10000\\,\\mathrm{MeV}$ disturb the plotting routine and averaging procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mc = 'opal_data/mc/'   # monte-carlo simulations\n",
    "\n",
    "### Open the file introducing file path\n",
    "ee = uproot.open(path_mc+'ee.root')\n",
    "mm = uproot.open(path_mc+'mm.root')\n",
    "tt = uproot.open(path_mc+'tt.root')\n",
    "qq = uproot.open(path_mc+'qq.root')\n",
    "ttree_name = 'myTTree'\n",
    "\n",
    "### Get list of 'branches' of the TTree (i.e. list of variable names)\n",
    "keys = ee[ttree_name].keys()\n",
    "\n",
    "## Load branches\n",
    "ee_branches = ee[ttree_name].arrays()\n",
    "mm_branches = mm[ttree_name].arrays()\n",
    "tt_branches = tt[ttree_name].arrays()\n",
    "qq_branches = qq[ttree_name].arrays()\n",
    "\n",
    "## Convert to pandas data frame\n",
    "simulated_electron = ak.to_pandas(ee_branches)\n",
    "simulated_muon = ak.to_pandas(mm_branches)\n",
    "simulated_tau = ak.to_pandas(tt_branches)\n",
    "simulated_quark = ak.to_pandas(qq_branches)\n",
    "\n",
    "## Efficiency correction factor (between nominal 100,000 events and the actual number of events)\n",
    "eff_corr_electons = len(simulated_electron) / 1e5\n",
    "eff_corr_muons = len(simulated_muon) / 1e5\n",
    "eff_corr_taus = len(simulated_tau) / 1e5\n",
    "eff_corr_quarks = len(simulated_quark) / 1e5\n",
    "\n",
    "## Delete photon-photon entries\n",
    "simulated_electron = simulated_electron[simulated_electron['Pcharged']!=0]\n",
    "simulated_muon = simulated_muon[simulated_muon['Pcharged']!=0]\n",
    "simulated_tau = simulated_tau[simulated_tau['Pcharged']!=0]\n",
    "simulated_quark = simulated_quark[simulated_quark['Pcharged']!=0]\n",
    "\n",
    "## Iterable arrays of different event types\n",
    "simulated_datasets = {'electron': simulated_electron, 'muon': simulated_muon, 'tau': simulated_tau, 'quark': simulated_quark}\n",
    "efficiency_corrections = {'electron': eff_corr_electons, 'muon': eff_corr_muons, 'tau': eff_corr_taus, 'quark': eff_corr_quarks}\n",
    "\n",
    "## Use correct NaN specifier for undefined values\n",
    "for particle, dataset in simulated_datasets.items():\n",
    "    dataset['cos_thru'].replace(999, np.nan, inplace=True)\n",
    "    dataset['cos_thet'].replace(999, np.nan, inplace=True)\n",
    "    dataset['Pcharged'].mask(dataset['Pcharged']>100, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the dataset\n",
    "\n",
    "We can now look at the ditribution of values for the physical quantities of the dataset.\n",
    "\n",
    "For that, we plot histograms of all keys in the data frame except ``run`` and ``event`` number and the LEP beam energy, which for those samples was always constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant variables within the sample\n",
    "relevant_variables = ['Ncharged', 'Pcharged', 'E_ecal', 'E_hcal', 'cos_thru', 'cos_thet']\n",
    "\n",
    "# Show histograms for each dataset with pandas\n",
    "if False:   # set True to print all the histograms\n",
    "    for particle, dataset in simulated_datasets.items():\n",
    "        print(particle)\n",
    "        diagram_list = dataset.hist(bins=50, column=relevant_variables)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better visualization of the parameter space, we can also plot each set of two variables in a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get the default matplotlib color cycle as indexable array (needed to later prepare the legend)\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "default_colors = prop_cycle.by_key()['color']\n",
    "\n",
    "\n",
    "## plot all combinations of interesting variables except for the angles\n",
    "if False:\n",
    "    for i in range(4):   # variable to be plotted on the x axis\n",
    "        for j in range(i+1, 4):   # variable to be plotted on the y axis\n",
    "\n",
    "            # plot faint datapoints\n",
    "            for k, (particle, dataset) in enumerate(simulated_datasets.items()):\n",
    "                plt.plot(dataset[relevant_variables[i]], dataset[relevant_variables[j]], '.', ms=0.2, alpha=0.1)   # actual data\n",
    "                plt.plot([], [], '.', color=default_colors[k], ms=5, alpha=1, label=particle)   # better visible dots for legend\n",
    "\n",
    "            plt.xlabel(relevant_variables[i])\n",
    "            plt.ylabel(relevant_variables[j])\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring cuts\n",
    "\n",
    "Separating the quark contributions is easily possible by their large number of charged traces:\n",
    "\n",
    "We declare all events with `Ncharged >= 10` to be quark events, all with `Ncharged <=6` to be non-quark events.\n",
    "\n",
    "For the leptons we could see the largest spread in the `Pcharged`-`E_ecal` 2D plot, there we choose regions differenciate the three types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare cuts\n",
    "Nmin_quarks = 10\n",
    "Nmax_leptons = 6\n",
    "Emin_ele = 80\n",
    "Emax_muons = 20\n",
    "Pmin_muons = 75\n",
    "Emax_taus = 60\n",
    "Pmin_taus = 5\n",
    "Pmax_taus = 55\n",
    "\n",
    "\n",
    "\n",
    "## plot histogram\n",
    "histogram_data = [simulated_electron['Ncharged'],simulated_muon['Ncharged'],simulated_tau['Ncharged'],simulated_quark['Ncharged']]\n",
    "plt.hist(histogram_data, histtype='step', bins=48, color=default_colors[:4])   # outline\n",
    "plt.hist(histogram_data, histtype='stepfilled', bins=48, alpha=0.25, color=default_colors[:4])   # filling\n",
    "plt.plot([Nmin_quarks, Nmin_quarks], [0, 1e4], c='m', ls=':', lw=2.5)  # plot separating lines\n",
    "plt.plot([Nmax_leptons+1, Nmax_leptons+1], [0, 1e4], c='m', ls=':', lw=2.5)  # plot separating lines\n",
    "plt.plot()\n",
    "plt.xlim(4,15)\n",
    "plt.ylim(0,2500)\n",
    "#plt.savefig('figures/selection_histogram.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## scatter plot\n",
    "for k, (particle, dataset) in enumerate(simulated_datasets.items()):\n",
    "    if particle != 'quark':\n",
    "        plt.plot(dataset['Pcharged'], dataset['E_ecal'], '.', ms=0.2, alpha=0.1)   # actual data\n",
    "        plt.plot([], [], '.', color=default_colors[k], ms=5, alpha=1, label=particle)   # better visible dots for legend\n",
    "    \n",
    "    ## visualize cuts by plotting separating lines\n",
    "    plt.plot([0, 100], [Emin_ele, Emin_ele], c='b', ls='--', lw=1.5)\n",
    "    plt.plot([Pmin_muons, Pmin_muons], [0, Emax_muons], c='y', ls='--', lw=1.5)\n",
    "    plt.plot([Pmin_muons, 100], [Emax_muons, Emax_muons], c='y', ls='--', lw=1.5)\n",
    "    plt.plot([Pmin_taus, Pmax_taus], [Emax_taus, Emax_taus], c='g', ls='--', lw=1.5)\n",
    "    plt.plot([Pmin_taus, Pmin_taus], [0, Emax_taus], c='g', ls='--', lw=1.5)\n",
    "    plt.plot([Pmax_taus, Pmax_taus], [0, Emax_taus], c='g', ls='--', lw=1.5)\n",
    "\n",
    "plt.xlabel('Pcharged')\n",
    "plt.ylabel('E_ecal')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0,120)\n",
    "#plt.savefig('figures/selection_scatter.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## define functions to produce the cutting mask on any dataset\n",
    "\n",
    "def electron_mask(dataset):\n",
    "    return np.all((dataset['Ncharged'] <= Nmax_leptons, dataset['E_ecal'] >= Emin_ele), axis=0)\n",
    "\n",
    "def muon_mask(dataset):\n",
    "    return np.all((dataset['Ncharged'] <= Nmax_leptons, dataset['E_ecal'] <= Emax_muons, dataset['Pcharged'] >= Pmin_muons), axis=0)\n",
    "\n",
    "def tau_mask(dataset):\n",
    "    return np.all((dataset['Ncharged'] <= Nmax_leptons, dataset['E_ecal'] <= Emax_taus,\n",
    "                   dataset['Pcharged'] >= Pmin_taus, dataset['Pcharged'] <= Pmax_taus), axis=0)\n",
    "\n",
    "def quark_mask(dataset):\n",
    "    return dataset['Ncharged'] >= Nmin_quarks\n",
    "\n",
    "## Iterable array of different selection types\n",
    "selection_masks = {'electron': electron_mask, 'muon': muon_mask, 'tau': tau_mask, 'quark': quark_mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying those cuts to the monte carlo data gives us the selection efficiencies; via matrix inversion the correcting factors for the measured data can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate and save selection rates\n",
    "selection_rates = np.zeros((4,4))\n",
    "selection_errors = np.zeros((4,4))\n",
    "\n",
    "for i, (particle, dataset) in enumerate(simulated_datasets.items()):\n",
    "    for j, (rule_particle, selection_rule) in enumerate(selection_masks.items()):\n",
    "        mask = selection_rule(dataset)\n",
    "        selection_rates[i][j] = sum(mask)/len(mask) * efficiency_corrections[particle]\n",
    "        selection_errors[i][j] = np.sqrt(sum(mask))/len(mask) * efficiency_corrections[particle]\n",
    "        print(f\"Out of {len(mask)} \" + particle + f\"-events, {np.sum(mask)} passed the \" + rule_particle + \"-selection mask\")\n",
    "        \n",
    "print('Selection rates: \\n', selection_rates)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gaussian function to fit to the toy distributions:\n",
    "def gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "\n",
    "### Number of toy experiments to be done\n",
    "ntoy = 100000\n",
    "\n",
    "### Create numpy matrix of list to append elements of inverted toy matrices\n",
    "inverse_toys = np.empty((4,4,ntoy))\n",
    "\n",
    "# Create toy efficiency matrix out of gaussian-distributed random values\n",
    "for i in range(ntoy):\n",
    "    toy_matrix = np.random.normal(selection_rates, selection_errors, size=(4,4))\n",
    "    \n",
    "    ### Invert toy matrix\n",
    "    inverse_toys[:,:,i] = np.linalg.inv(toy_matrix)\n",
    "\n",
    "\n",
    "inverse_errors = np.zeros((4,4))\n",
    "inverse_means = np.zeros((4,4))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 10),dpi=80)\n",
    "\n",
    "\n",
    "# Fill histograms for each inverted matrix coefficient:\n",
    "for j in range(4):\n",
    "    for k in range(4):\n",
    "        \n",
    "        ## Calculate empirical mean and standard deviation directly\n",
    "        mean = np.mean(inverse_toys[j,k,:])\n",
    "        std = np.sqrt(np.sum((inverse_toys[j,k,:] - mean)**2 )/ (ntoy-1))\n",
    "        \n",
    "        # Diagonal and off-diagonal terms have different histogram ranges\n",
    "        hbins, hedges, _ = axes[j][k].hist(inverse_toys[j,k,:], bins=300, histtype='step', linewidth=2,\n",
    "                                           range=(mean-5*std, mean+5*std),  label=f'toyhist{j}{k}')\n",
    "        axes[j][k].legend()\n",
    "\n",
    "        # Get the fitted curve\n",
    "        h_mid = 0.5*(hedges[1:] + hedges[:-1]) # Calculate midpoints for the fit\n",
    "        coeffs, _ = curve_fit(gauss, h_mid, hbins, maxfev=10000, p0=(1, mean, std))\n",
    "        \n",
    "        axes[j][k].plot(h_mid, gauss(h_mid, *coeffs), label=f'Fit{j}{k}')\n",
    "\n",
    "        inverse_means[j,k] = coeffs[1]\n",
    "        inverse_errors[j,k] = abs(coeffs[2])\n",
    "\n",
    "print(f\"Errors for the inverse matrix:\\n{inverse_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import OPAL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open opal data and read into pandas\n",
    "path_OPAL = 'opal_data/data/'   # actual data  simulations\n",
    "OPAL = uproot.open(path_OPAL+'daten_2.root')\n",
    "OPAL_branches = OPAL[ttree_name].arrays()   # load branches\n",
    "OPAL_data = ak.to_pandas(OPAL_branches)   # convert to pandas data frame\n",
    "OPAL_data = OPAL_data[OPAL_data['Pcharged']!=0]   # remove photon-photon contributions\n",
    "\n",
    "## Use correct NaN specifier for undefined values\n",
    "OPAL_data['cos_thru'].replace(999, np.nan, inplace=True)\n",
    "OPAL_data['cos_thet'].replace(999, np.nan, inplace=True)\n",
    "OPAL_data['Pcharged'].mask(OPAL_data['Pcharged']>100, inplace=True)\n",
    "\n",
    "## extract different event types\n",
    "measured_electron = OPAL_data[electron_mask(OPAL_data)]\n",
    "measured_muon = OPAL_data[muon_mask(OPAL_data)]\n",
    "measured_tau = OPAL_data[tau_mask(OPAL_data)]\n",
    "measured_quark = OPAL_data[quark_mask(OPAL_data)]\n",
    "\n",
    "## Iterable array of different event types\n",
    "measured_datasets = {'electron': measured_electron, 'muon': measured_muon, 'tau': measured_tau, 'quark': measured_quark}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPAL_data.hist(bins=52)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Separate $t$- and $s$-channel contributions\n",
    "\n",
    "Only Feynman diagrams contributing to the production of $Z$ boson are to be considered for the measurements. The **electron** Monte Carlo sample incorporate contributions from $t$- and $s$-channels.\n",
    "* Select/correct contributions producing $Z$ boson decays. (Hint: Which role does the $\\cos(\\theta)$ distribution play in separating $t$- and $s$-channels?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define fit function\n",
    "def func_t_s_separation(cos_theta, A, B):\n",
    "    return A * (1 + cos_theta**2) + B * (1 - cos_theta)**(-2)\n",
    "\n",
    "def func_s_channel(cos_theta, A, B):\n",
    "    return A * (1 + cos_theta**2)\n",
    "\n",
    "def func_rest(cos_theta, A, B):\n",
    "    return B * (1 - cos_theta)**(-2)\n",
    "\n",
    "\n",
    "# this function performs the separation via a curve fit\n",
    "def perform_t_s_separation(the_input_dataset, make_plot=True, bins=100, limit=0.85):\n",
    "    # create histogram from given dataset\n",
    "    electron_cos_theta, bin_edges, _ = plt.hist(the_input_dataset['cos_thet'],bins=bins, range=(-1.0,1.0),  histtype='stepfilled', label='Pcharged')\n",
    "    \n",
    "    # calculate bin centers\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    if make_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.clf()   #plt.show() if you also want to view the histogram\n",
    "    \n",
    "    # mask bins in desired region for curve fit\n",
    "    curve_fit_mask = np.logical_and(bin_edges[:-1] >= -limit, bin_edges[1:] <= limit)\n",
    "    \n",
    "    # calculate curve fit in desired region\n",
    "    t_s_popt, t_s_pcov = curve_fit(func_t_s_separation, bin_centers[curve_fit_mask], electron_cos_theta[curve_fit_mask], sigma=np.sqrt(electron_cos_theta[curve_fit_mask]), maxfev=int(1e6), p0=[2, .2])\n",
    "    t_s_perr = np.sqrt(np.diag(t_s_pcov))\n",
    "    \n",
    "    thetas = np.linspace(-limit, limit, 10000)\n",
    "    # plot results if desired\n",
    "    if make_plot:\n",
    "        textstr = '\\n'.join((\n",
    "            r'$P_\\mathrm{meas} = a \\cdot P_\\mathrm{nom}+b$',\n",
    "            r'$A=(%.3f \\pm %.3f)$' % (t_s_popt[0], t_s_perr[0]),\n",
    "            r'$B=(%.2f \\pm %.2f)$' % (t_s_popt[1], t_s_perr[1])))\n",
    "\n",
    "\n",
    "        props = dict(alpha=1, facecolor='w')\n",
    "\n",
    "        plt.errorbar(bin_centers, electron_cos_theta, yerr=np.sqrt(electron_cos_theta), fmt='.', ms=3, ecolor='r', elinewidth=1)\n",
    "        plt.plot(thetas, func_t_s_separation(thetas, *t_s_popt),lw=0.7, c='purple', label='entire fit')\n",
    "        plt.plot(thetas, func_s_channel(thetas, *t_s_popt), lw=0.7, c='lime', label='s-channel contribution')\n",
    "        plt.plot(thetas, func_rest(thetas, *t_s_popt), lw=0.7, c='cyan', label='t-channel and interference')\n",
    "        #plt.text(-0.9, 2000, textstr, fontsize=10, verticalalignment='top', bbox=props)\n",
    "\n",
    "        plt.xlabel('polar angle $\\mathrm{cos}\\,(\\\\theta)$')\n",
    "        plt.ylabel('counts')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # determine s-channel event number\n",
    "    electron_s_channel = np.sum(electron_cos_theta * (t_s_popt[0] * (1 + bin_centers**2)) / ((t_s_popt[0] * (1 + bin_centers**2)) + t_s_popt[1] / (1 - bin_centers)**2))\n",
    "    # propagate individual bin uncertainty (i.e. Poisson uncertainty)\n",
    "    electron_s_channel_u_1 = np.sqrt(np.sum((np.sqrt(electron_cos_theta) * (t_s_popt[0] * (1 + bin_centers**2)) / ((t_s_popt[0] * (1 + bin_centers**2)) + t_s_popt[1] / (1 - bin_centers)**2))**2))\n",
    "    # propagate fit parameter uncertainty/covariance\n",
    "    t_s_ddA = np.sum(electron_cos_theta * (t_s_popt[1] * (1+bin_centers**2)/((1-bin_centers)**2))/((t_s_popt[0] * (1+bin_centers**2) + t_s_popt[1] * (1+bin_centers**2)/((1-bin_centers)**2))**2))\n",
    "    t_s_ddB = np.sum(electron_cos_theta * (t_s_popt[0] * (1+bin_centers**2)/((1-bin_centers)**2))/((t_s_popt[0] * (1+bin_centers**2) + t_s_popt[1] * (1+bin_centers**2)/((1-bin_centers)**2))**2))\n",
    "    electron_s_channel_u_2 = np.sqrt((t_s_ddA * np.sqrt(t_s_pcov[0,0]))**2 + (t_s_ddB * np.sqrt(t_s_pcov[1,1]))**2 + 2 * t_s_ddA * t_s_ddB * t_s_pcov[0,1])\n",
    "    # combine uncertainties\n",
    "    electron_s_channel_u = np.sqrt(electron_s_channel_u_1**2 + electron_s_channel_u_2**2)\n",
    "    \n",
    "    # UPDATE - Correct for unknown angles:\n",
    "    # There exist a certain fraction of events for which 'cos_theta' is not defined probably due to the detector electronics\n",
    "    # being unable to reconstruct the angle properly. By creating a histogram of the 'cos_theta' values, these events are \n",
    "    # automatically excluded from this ee event evaluation even though they are proper ee events according to the cuts we\n",
    "    # chose above. To correct for these lost events, we multiply the resulting s-channel event number by the appropriate\n",
    "    # factor (number of all ee events)/(number of ee events with proper 'cos_theta' value). In doing so, we assume that the\n",
    "    # detector has no bias, i.e. the 'cos_theta' value cannot be reconstructed for a constant fraction of ee events no matter\n",
    "    # the real (unknown) angle.\n",
    "    factor = len(the_input_dataset) / np.sum(electron_cos_theta)\n",
    "    electron_s_channel *= factor\n",
    "    electron_s_channel_u *= factor\n",
    "    \n",
    "    # print results\n",
    "    print(\"fit params:     \", t_s_popt[0], t_s_popt[1])\n",
    "    print(\"total ee events:\", np.sum(electron_cos_theta), \"+-\", np.sqrt(np.sum(electron_cos_theta)))\n",
    "    print(\"s-channel only: \", electron_s_channel, \"+-\", electron_s_channel_u)\n",
    "    \n",
    "    return electron_s_channel, electron_s_channel_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply t-s-selection to electron events\n",
    "\n",
    "# prepare arrays for events (7 different LEP_energies, 4 different event categories)\n",
    "orig_events = np.zeros((7,4))\n",
    "orig_events_u = np.zeros((7,4))\n",
    "\n",
    "# get the seven different LEP-energies\n",
    "LEP_energies = np.sort(OPAL_data['E_lep'].round(1).unique())\n",
    "\n",
    "# do for all energy values\n",
    "for e, energy in enumerate(LEP_energies):\n",
    "    \n",
    "    # calculate electron s-channel event numbers\n",
    "    s_channel_only, s_channel_only_u = perform_t_s_separation(measured_electron[measured_electron['E_lep'].round(1) == energy], bins=20)\n",
    "    \n",
    "    # write to event arrays\n",
    "    orig_events[e][0] = s_channel_only\n",
    "    orig_events_u[e][0] = s_channel_only_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Measurement of the total production cross sections\n",
    "\n",
    "For **each** of the seven centre-of-mass energies:\n",
    "* Determine the number of events in the handronic channel *and* in the three leptonic channels\n",
    "* Substract the background and correct for selection efficiencies accordingly\n",
    "* Then, calculate the differnetial cross sections for the hadronic *and* the leptnic channels\n",
    "* Add the radiation corrections from The table given below. **Don't forget to take the uncertainties (errors) into account!**\n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Correction hadronic channel    \\[nb\\] |  Correction leptonic channel   \\[nb\\]|\n",
    "| --- | --- | --- |\n",
    "| 88.47 | +2.0  | +0.09 |\n",
    "| 89.46 | +4.3  | +0.20 |\n",
    "| 90.22 | +7.7  | +0.36 |\n",
    "| 91.22 | +10.8 | +0.52 |\n",
    "| 91.97 | +4.7  | +0.22 |\n",
    "| 92.96 | -0.2  | -0.01 |\n",
    "| 93.76 | -1.6  | -0.08 |\n",
    "\n",
    "Feel free to access these values using the dictionary 'xs_corrections' given below.\n",
    "* Once the total cross section for all four decay channels at all seven energies have been measured, fit a **Breit-Wigner distribution** to measure the $Z$ boson mass ($m_Z$) and the resonance width ($\\Gamma_Z$) and the peak cross section s of the resonance for the hadronic and the leptonic channels. Again, **propagate the uncertainties carefully**.\n",
    "* Compare your results to the OPAL cross section s and the theoretical predictions. How many degrees of freedom does the fit have? How can you udge if the model is compatible with the measured data? Calculate the  **confidence levels**.\n",
    "* Calculate the partial widths for all channels from the measured cross sections on the peak. Which is the best partial width to start with? Compare them with the theoretical predictions and the values that you have calculated in the beginning.\n",
    "* Determine from your results the **number of generations of light neutrinos**. Which assumptions are necessary?\n",
    "* Discuss in detail the systematic uncertainties in the whole procedure of the analysis. Which assumptions were necessary?\n",
    "\n",
    "These are some **references** that might be interesting to look up:\n",
    "* Particle Data Book: https://pdg.lbl.gov/2020/download/Prog.Theor.Exp.Phys.2020.083C01.pdf\n",
    "** Resonances: https://pdg.lbl.gov/2017/reviews/rpp2017-rev-resonances.pdf\n",
    "* Precision Electroweak Measurements on the Z Resonance (Combination LEP): https://arxiv.org/abs/hep-ex/0509008\n",
    "* Measurement of the $Z^0$ mass and width with the OPAL detector at LEP: https://doi.org/10.1016/0370-2693(89)90705-3\n",
    "* Measurement of the $Z^0$ line shape parameters and the electroweak couplings of charged leptons: https://inspirehep.net/literature/315269\n",
    "* The OPAL Collaboration, *Precise Determination of the $Z$ Resonance Parameters at LEP: \"Zedometry\"*: https://arxiv.org/abs/hep-ex/0012018\n",
    "* Fitting a Breit-Wigner curve using uproot: https://masonproffitt.github.io/uproot-tutorial/07-fitting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_corrections = pd.DataFrame({ 'energy' : [ 88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                              'hadronic' : [2.0, 4.3, 7.7, 10.8, 4.7, -0.2, -1.6],\n",
    "                              'leptonic' : [0.09, 0.20, 0.36, 0.52, 0.22, -0.01, -0.08]})\n",
    "\n",
    "## read csv-file of luminosity data\n",
    "file = 'opal_data/lumi_files/daten_2.csv'\n",
    "lumi_data = pd.read_csv(file)\n",
    "\n",
    "# print lumi data\n",
    "lumi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cross sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the number of measured events for other particles (muon, tau, hadrons)\n",
    "\n",
    "# do for all event types and all energy values\n",
    "for p, (particle, dataset) in enumerate(measured_datasets.items()):\n",
    "    # we already did electrons in the cell above\n",
    "    if p == 0:\n",
    "        continue\n",
    "    \n",
    "    for e, energy in enumerate(LEP_energies):\n",
    "        \n",
    "        # count events and write to event arrays\n",
    "        orig_events[e][p] = np.sum(dataset['E_lep'].round(1) == energy)\n",
    "        \n",
    "        # calculate Poisson uncertainty\n",
    "        orig_events_u[e][p] = np.sqrt(orig_events[e][p])\n",
    "    \n",
    "    \n",
    "# print data\n",
    "pd.DataFrame(orig_events, columns = ['e_events', 'm_events', 't_events', 'q_events'])\n",
    "#pd.DataFrame(orig_events_u, columns = ['e_events_u', 'm_events_u', 't_events_u', 'q_events_u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correct the measured events using the inverted efficiency matrix\n",
    "\n",
    "# prepare array\n",
    "corr_events = np.zeros((7,4))\n",
    "corr_events_u = np.zeros((7,4))\n",
    "\n",
    "# do for all energy values\n",
    "for e in range(7):\n",
    "    \n",
    "    # calculate \"true\" event numbers using inverted efficiency matrix\n",
    "    corr_events[e] = inverse_means @ orig_events[e]\n",
    "    \n",
    "    # propagate uncertainties (cannot be done in form of matrix multiplication because of the exponent 2)\n",
    "    for p in range(4):\n",
    "        \n",
    "        # propagate uncertainties from inverted matrix\n",
    "        u_correction_sum = np.sum((inverse_errors[p] * orig_events[e])**2)\n",
    "        \n",
    "        # propagate Poisson uncertainties\n",
    "        u_poisson_sum = np.sum((inverse_means[p] * orig_events_u[e])**2)\n",
    "        \n",
    "        # combine uncertainties\n",
    "        corr_events_u[e][p] = np.sqrt(u_correction_sum + u_poisson_sum)\n",
    "    \n",
    "        \n",
    "# print data\n",
    "pd.DataFrame(corr_events, columns = ['e_events', 'm_events', 't_events', 'q_events'])\n",
    "#pd.DataFrame(corr_events_u, columns = ['e_events_u', 'm_events_u', 't_events_u', 'q_events_u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate cross sections\n",
    "\n",
    "# prepare arrays\n",
    "cross_section = np.zeros((7,4))\n",
    "cross_section_u = np.zeros((7,4))\n",
    "\n",
    "# do for all energies\n",
    "for e in range(7):\n",
    "    # calculate cross section\n",
    "    cross_section[e] = corr_events[e] / lumi_data['lumi'][e]\n",
    "    \n",
    "    # propagate uncertainties\n",
    "    for p in range(4):\n",
    "        cross_section_u[e][p] = cross_section[e][p] * \\\n",
    "            np.sqrt((corr_events_u[e][p]/corr_events[e][p])**2 + (lumi_data['all'][e]/lumi_data['lumi'][e])**2)\n",
    "\n",
    "\n",
    "## apply radiation corrections\n",
    "for e in range(7):\n",
    "    \n",
    "    # correct lepton data\n",
    "    cross_section[e][0] += xs_corrections['leptonic'][e]\n",
    "    cross_section[e][1] += xs_corrections['leptonic'][e]\n",
    "    cross_section[e][2] += xs_corrections['leptonic'][e]\n",
    "    \n",
    "    # correct hadron data\n",
    "    cross_section[e][3] += xs_corrections['hadronic'][e]\n",
    "        \n",
    "# print data\n",
    "pd.DataFrame(cross_section, columns=['sigma_ee', 'sigma_mm', 'sigma_tt', 'sigma_qq'])\n",
    "#pd.DataFrame(cross_section_u, columns=['sigma_ee_u', 'sigma_mm_u', 'sigma_tt_u', 'sigma_qq_u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot cross sections to energies\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p, particle in enumerate(['$\\\\mathrm{Z}^0\\\\rightarrow\\\\mathrm{e}^+\\\\mathrm{e}^-$',\n",
    "                             '$\\\\mathrm{Z}^0\\\\rightarrow\\\\mu^+\\\\mu^-$',\n",
    "                             '$\\\\mathrm{Z}^0\\\\rightarrow\\\\tau^+\\\\tau^-$',\n",
    "                             '$\\\\mathrm{Z}^0\\\\rightarrow\\\\mathrm{q}\\\\,\\\\bar{\\\\mathrm{q}}$']):\n",
    "    plt.errorbar(2*LEP_energies, cross_section[:,p], yerr=cross_section_u[:,p], fmt=\"x\", label=particle)\n",
    "\n",
    "plt.xlabel(\"center-of-mass energy $\\\\sqrt{s}$ [$\\\\mathrm{GeV}$]\")\n",
    "plt.ylabel(\"partial cross section $\\\\sigma$ [$\\\\mathrm{nb}$]\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 2.2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate cross sections with fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit Breit-Wigner functions\n",
    "\n",
    "# define Breit-Wigner function\n",
    "def breit_wigner(s, mass, gamma, peak):\n",
    "    return peak * s / (((s - mass**2)**2 / gamma**2) + (s**2 / mass**2))\n",
    "\n",
    "# define Gaussian function\n",
    "def gaussian(x, x_0, sigma, a):\n",
    "    return a * np.exp(-(x-x_0)**2 / (2*sigma**2))\n",
    "\n",
    "\n",
    "# this function performs the breit wigner fit for the given particle index\n",
    "def perform_breit_wigner_fits(particle_index, repetitions=int(1e3)):\n",
    "    particle = particle_index\n",
    "\n",
    "    LEP_sqrt_s = 2*LEP_energies\n",
    "\n",
    "    # prepare arrays\n",
    "    bw_popts = np.zeros((repetitions, 3))\n",
    "    fit_cross_sections = np.repeat([cross_section[:,particle]], repetitions, axis=0)\n",
    "\n",
    "    # add normal distributed deviations\n",
    "    for e in range(7):\n",
    "        fit_cross_sections[:,e] += np.random.normal(0.0, cross_section_u[e,particle], repetitions)\n",
    "\n",
    "    # fit a lot of times\n",
    "    for i in range(repetitions):\n",
    "\n",
    "        # perform curve fit\n",
    "        bw_popt, bw_pcov = curve_fit(breit_wigner, LEP_sqrt_s**2, fit_cross_sections[i], p0=[91, 3, 40], maxfev=int(1e6))\n",
    "\n",
    "        # store results\n",
    "        bw_popts[i] = bw_popt\n",
    "\n",
    "        # plot if desired\n",
    "        energies = np.linspace(np.amin(LEP_sqrt_s)-.5, np.amax(LEP_sqrt_s)+.5, int(1e4))\n",
    "        plt.plot(energies, breit_wigner(energies**2, *bw_popt), c=\"#888888\")\n",
    "\n",
    "    # plot original cross sections on top of fits\n",
    "    plt.errorbar(LEP_sqrt_s, cross_section[:,particle], yerr=cross_section_u[:,particle], fmt=\"x\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # calculate mean fit parameters\n",
    "    bw_popt = np.mean(bw_popts, axis=0)\n",
    "    \n",
    "    # calculate covariances\n",
    "    bw_pcov = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(i, 3):\n",
    "            bw_pcov[i][j] = bw_pcov[j][i] = np.sum((bw_popts[:,i]-bw_popt[i]) * (bw_popts[:,j]-bw_popt[j]))/(repetitions-1)\n",
    "\n",
    "    # print results\n",
    "    print(bw_popt[0], \"+-\", np.sqrt(bw_pcov[0,0]))\n",
    "    print(bw_popt[1], \"+-\", np.sqrt(bw_pcov[1,1]))\n",
    "    print(bw_popt[2], \"+-\", np.sqrt(bw_pcov[2,2]))\n",
    "\n",
    "            \n",
    "    # return fit params and covariances\n",
    "    return bw_popt, bw_pcov\n",
    "\n",
    "\n",
    "\n",
    "# fit electron events\n",
    "ee_popt, ee_pcov = perform_breit_wigner_fits(0)\n",
    "\n",
    "# fit muon events\n",
    "mm_popt, mm_pcov = perform_breit_wigner_fits(1)\n",
    "\n",
    "# fit tau events\n",
    "tt_popt, tt_pcov = perform_breit_wigner_fits(2)\n",
    "\n",
    "# fit hadron events\n",
    "qq_popt, qq_pcov = perform_breit_wigner_fits(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine partial decay width for electron-positron events\n",
    "\n",
    "# unit factor nb^(1/2) to MeV^(-1)\n",
    "unit_factor = np.sqrt(2.56819)\n",
    "# unit factor nb to GeV^(-2)\n",
    "unit_factor2 = 2.56819e-6\n",
    "# unit factor GeV^(2)/MeV to MeV\n",
    "unit_factor3 = 1e6\n",
    "\n",
    "# do calculation\n",
    "partial_ee = unit_factor * np.sqrt(ee_popt[2] / (12*np.pi)) * ee_popt[0] * ee_popt[1]\n",
    "\n",
    "# propagate uncertainties and covariances\n",
    "ddPeak = ee_popt[0] * ee_popt[1] / (2 * np.sqrt(12 * np.pi * ee_popt[2]))\n",
    "ddMass = ee_popt[1] * np.sqrt(ee_popt[2] / (12*np.pi))\n",
    "ddGamma = ee_popt[0] * np.sqrt(ee_popt[2] / (12*np.pi))\n",
    "partial_ee_u = unit_factor * np.sqrt(ddMass**2 * ee_pcov[0,0] + ddGamma**2 * ee_pcov[1,1] + ddPeak**2 * ee_pcov[2,2] + \\\n",
    "                       2 * ddMass * ddGamma * ee_pcov[0,1] + 2 * ddMass * ddPeak * ee_pcov[0,2] + \\\n",
    "                       2 * ddGamma * ddPeak * ee_pcov[1,2])\n",
    "# print result\n",
    "print(\"Gamma_ee: \", partial_ee, \"+-\", partial_ee_u, \"MeV\")\n",
    "\n",
    "\n",
    "\n",
    "## determine partial decay width for muon events\n",
    "partial_mm = unit_factor2 * unit_factor3 * mm_popt[0]**2 * mm_popt[1]**2 * mm_popt[2] / (12 * np.pi * partial_ee)\n",
    "\n",
    "# propagate uncertainties and covariances\n",
    "ddPeak = mm_popt[0]**2 * mm_popt[1]**2 / (12*np.pi * partial_ee)\n",
    "ddMass = mm_popt[0] * mm_popt[1]**2 * mm_popt[2] / (6*np.pi * partial_ee)\n",
    "ddGamma = mm_popt[0]**2 * mm_popt[1] * mm_popt[2] / (6*np.pi * partial_ee)\n",
    "ddPartial_ee = -mm_popt[2] * mm_popt[0]**2 * mm_popt[1]**2 / (12*np.pi * partial_ee**2)\n",
    "partial_mm_u = unit_factor2 * unit_factor3 * np.sqrt(ddMass**2 * mm_pcov[0,0] + ddGamma**2 * mm_pcov[1,1] + \\\n",
    "                                                     ddPeak**2 * mm_pcov[2,2] + 2 * ddMass * ddGamma * mm_pcov[0,1] + \\\n",
    "                                                     2 * ddMass * ddPeak * mm_pcov[0,2] + 2 * ddGamma * ddPeak * mm_pcov[1,2] + \\\n",
    "                                                     ddPartial_ee**2 * partial_ee_u**2)\n",
    "# print result\n",
    "print(\"Gamma_mm: \", partial_mm, \"+-\", partial_mm_u, \"MeV\")\n",
    "\n",
    "\n",
    "\n",
    "## determine partial decay width for tau events\n",
    "partial_tt = unit_factor2 * unit_factor3 * tt_popt[0]**2 * tt_popt[1]**2 * tt_popt[2] / (12 * np.pi * partial_ee)\n",
    "\n",
    "# propagate uncertainties and covariances\n",
    "ddPeak = tt_popt[0]**2 * tt_popt[1]**2 / (12*np.pi * partial_ee)\n",
    "ddMass = tt_popt[0] * tt_popt[1]**2 * tt_popt[2] / (6*np.pi * partial_ee)\n",
    "ddGamma = tt_popt[0]**2 * tt_popt[1] * tt_popt[2] / (6*np.pi * partial_ee)\n",
    "ddPartial_ee = -tt_popt[2] * tt_popt[0]**2 * tt_popt[1]**2 / (12*np.pi * partial_ee**2)\n",
    "partial_tt_u = unit_factor2 * unit_factor3 * np.sqrt(ddMass**2 * tt_pcov[0,0] + ddGamma**2 * tt_pcov[1,1] + \\\n",
    "                                                     ddPeak**2 * tt_pcov[2,2] + 2 * ddMass * ddGamma * tt_pcov[0,1] + \\\n",
    "                                                     2 * ddMass * ddPeak * tt_pcov[0,2] + 2 * ddGamma * ddPeak * tt_pcov[1,2] + \\\n",
    "                                                     ddPartial_ee**2 * partial_ee_u**2)\n",
    "# print result\n",
    "print(\"Gamma_tt: \", partial_tt, \"+-\", partial_tt_u, \"MeV\")\n",
    "\n",
    "\n",
    "\n",
    "## determine partial decay width for hadron events\n",
    "partial_qq = unit_factor2 * unit_factor3 * qq_popt[0]**2 * qq_popt[1]**2 * qq_popt[2] / (12 * np.pi * partial_ee)\n",
    "\n",
    "# propagate uncertainties and covariances\n",
    "ddPeak = qq_popt[0]**2 * qq_popt[1]**2 / (12*np.pi * partial_ee)\n",
    "ddMass = qq_popt[0] * qq_popt[1]**2 * qq_popt[2] / (6*np.pi * partial_ee)\n",
    "ddGamma = qq_popt[0]**2 * qq_popt[1] * qq_popt[2] / (6*np.pi * partial_ee)\n",
    "ddPartial_ee = -qq_popt[2] * qq_popt[0]**2 * qq_popt[1]**2 / (12*np.pi * partial_ee**2)\n",
    "partial_qq_u = unit_factor2 * unit_factor3 * np.sqrt(ddMass**2 * qq_pcov[0,0] + ddGamma**2 * qq_pcov[1,1] + \\\n",
    "                                                     ddPeak**2 * qq_pcov[2,2] + 2 * ddMass * ddGamma * qq_pcov[0,1] + \\\n",
    "                                                     2 * ddMass * ddPeak * qq_pcov[0,2] + 2 * ddGamma * ddPeak * qq_pcov[1,2] + \\\n",
    "                                                     ddPartial_ee**2 * partial_ee_u**2)\n",
    "# print result\n",
    "print(\"Gamma_qq: \", partial_qq, \"+-\", partial_qq_u, \"MeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate breit wigner fit mean values\n",
    "\n",
    "# calculate z0 boson mass via weighted average\n",
    "z0_mass = (ee_popt[0]/ee_pcov[0,0] + mm_popt[0]/mm_pcov[0,0] + tt_popt[0]/tt_pcov[0,0] + qq_popt[0]/qq_pcov[0,0]) / \\\n",
    "            (1/ee_pcov[0,0] + 1/mm_pcov[0,0] + 1/tt_pcov[0,0] + 1/qq_pcov[0,0])\n",
    "# propagate uncertainty\n",
    "z0_mass_u = 1 / np.sqrt(1/ee_pcov[0,0] + 1/mm_pcov[0,0] + 1/tt_pcov[0,0] + 1/qq_pcov[0,0])\n",
    "\n",
    "# print result\n",
    "print(\"z0 mass:        \", z0_mass, \"+-\", z0_mass_u, \"GeV\")\n",
    "\n",
    "\n",
    "\n",
    "# calculate z0 boson total decay width via weighted average\n",
    "z0_decay_width = (ee_popt[1]/ee_pcov[1,1] + mm_popt[1]/mm_pcov[1,1] + tt_popt[1]/tt_pcov[1,1] + qq_popt[1]/qq_pcov[1,1]) / \\\n",
    "            (1/ee_pcov[1,1] + 1/mm_pcov[1,1] + 1/tt_pcov[1,1] + 1/qq_pcov[1,1])\n",
    "# propagate uncertainty\n",
    "z0_decay_width_u = 1 / np.sqrt(1/ee_pcov[1,1] + 1/mm_pcov[1,1] + 1/tt_pcov[1,1] + 1/qq_pcov[1,1])\n",
    "\n",
    "# print result\n",
    "print(\"z0 decay width: \", z0_decay_width, \"+-\", z0_decay_width_u, \"GeV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Forward-backward asymmetry and $\\sin^2(\\theta_\\text{W})$ in muon final states\n",
    "\n",
    "* Using the **muon channel only**, measure the forward-backward asymmetry $\\mathcal{A}_\\text{FB}$ using OPAL data and muon Monte Carlo events. Take into account the radiation corrections given below. \n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Radiation correction [-]|  \n",
    "| --- | --- | \n",
    "| 88.47 | 0.021512  | \n",
    "| 89.46 | 0.019262  | \n",
    "| 90.22 | 0.016713  | \n",
    "| 91.22 | 0.018293  | \n",
    "| 91.97 | 0.030286  | \n",
    "| 92.96 | 0.062196  | \n",
    "| 93.76 | 0.093850  | \n",
    "\n",
    "Feel free to use the dictionary 'radiation_corrections' given below.\n",
    "\n",
    "* Measure the **Weinberg angle** as $\\sin^2(\\theta_\\text{W})$. Compare the measurement with the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiation_corrections = pd.DataFrame({ 'energy' : [ 88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                                'correction' : [0.021512, 0.019262, 0.016713, 0.018293, 0.030286, 0.062196, 0.093850]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we perform the calculation of the weinberg angle for the monte carlo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the counts forwards and backwards\n",
    "forward_sim = sum(simulated_muon['cos_thet'] > 0)\n",
    "backward_sim = sum(simulated_muon['cos_thet'] < 0)\n",
    "\n",
    "# uncertainties according to poission\n",
    "f_sim_u = np.sqrt(forward_sim)\n",
    "b_sim_u = np.sqrt(backward_sim)\n",
    "\n",
    "# calculate forward-backward assym. and uncertainty\n",
    "AFB_sim = (forward_sim - backward_sim) / (forward_sim + backward_sim) + radiation_corrections['correction'][3]\n",
    "AFB_sim_u = np.sqrt( ((2*backward_sim)/(forward_sim + backward_sim)**2 * f_sim_u)**2 \n",
    "                    + ((2*forward_sim)/(forward_sim + backward_sim)**2 * b_sim_u)**2)\n",
    "\n",
    "# calculate weinberg angle and uncertainty\n",
    "sinsq_thetaW_sim = 0.25 - np.sqrt(AFB_sim/3) / 4\n",
    "sinsq_thetaW_sim_u = 1 / (8 * np.sqrt(3) * np.sqrt(AFB_sim)) * AFB_sim_u\n",
    "\n",
    "print('For MC muons: \\n \\t \\t sin²(theta) = ' + \n",
    "      str(np.round(sinsq_thetaW_sim, 4)) + ' +- ' +str(np.round(sinsq_thetaW_sim_u, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do the same for all the energies within the measured dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup arrays to store the results\n",
    "AFB_meas = np.zeros(7)\n",
    "AFB_meas_u = np.zeros(7)\n",
    "sinsq_thetaW_meas = np.zeros(7)\n",
    "sinsq_thetaW_meas_u = np.zeros(7)\n",
    "\n",
    "# do the recalculation for each LEP energy\n",
    "for e, energy in enumerate(LEP_energies):\n",
    "    \n",
    "    # extract only the events for that energy\n",
    "    dataset = measured_muon[measured_muon['E_lep'].round(1) == energy]\n",
    "\n",
    "    # calculate the counts forwards and backwards\n",
    "    forward_meas = sum(dataset['cos_thet'] > 0)\n",
    "    backward_meas = sum(dataset['cos_thet'] < 0)\n",
    "\n",
    "    # uncertainties according to poission\n",
    "    f_meas_u = np.sqrt(forward_meas)\n",
    "    b_meas_u = np.sqrt(backward_meas)\n",
    "\n",
    "    # calculate forward-backward assym. and uncertainty\n",
    "    AFB_meas[e] = (forward_meas - backward_meas) / (forward_meas + backward_meas) + radiation_corrections['correction'][e]\n",
    "    AFB_meas_u[e] = np.sqrt( ((2*backward_meas)/(forward_meas + backward_meas)**2 * f_meas_u)**2 \n",
    "                    + ((2*forward_meas)/(forward_meas + backward_meas)**2 * b_meas_u)**2)\n",
    "    \n",
    "    # calculate weinberg angle and uncertainty\n",
    "    sinsq_thetaW_meas[e] = 0.25 - np.sqrt(AFB_meas[e]/3) / 4\n",
    "    sinsq_thetaW_meas_u[e] = 1 / (8 * np.sqrt(3) * np.sqrt(AFB_meas[e])) * AFB_meas_u[e]\n",
    "    \n",
    "    if e==4:   # here, the approximation is exact\n",
    "        print('For measured muons at sqrt(s) = ' + str(radiation_corrections['energy'][e]) + ': \\n \\t \\t sin²(theta) = ' + \n",
    "              str(np.round(sinsq_thetaW_meas[e], 4)) + ' +- ' +str(np.round(sinsq_thetaW_meas_u[e], 4)))\n",
    "    \n",
    "plt.errorbar(radiation_corrections['energy'], AFB_meas, AFB_meas_u, fmt='x', ecolor='r')\n",
    "plt.xlabel(\"center-of-mass energy $\\\\sqrt{s}$ [$\\\\mathrm{GeV}$]\")\n",
    "plt.ylabel(\"forward backward assymetry $A_\\\\mathrm{FB}$\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S: For the `nan`-values, $A_\\mathrm{FB}$ was negative, so the square root failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Tests on lepton universality¶\n",
    "\n",
    "* Test the lepton universality from the total cross sectinos on the peak for $Z\\to e^+ e^-$, $Z\\to \\mu^+ \\mu^-$ and $Z\\to \\tau^+ \\tau^-$ events. What is the ratio of the total cross section of the hadronic channel to the leptonic channels on the peak? Compare with the ratios obtained from the branching rations and discuss possible differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate ratios between leptonic and hadronic cross section\n",
    "\n",
    "# prepare arrays\n",
    "lepton_to_hadron_ratios = np.zeros((7,3))\n",
    "lepton_to_hadron_ratios_u = np.zeros((7,3))\n",
    "\n",
    "# do for all energies\n",
    "for e in range(7):\n",
    "    # devide lepton cross sections by hadronic cross section\n",
    "    lepton_to_hadron_ratios[e] = cross_section[e,:-1] / cross_section[e,-1]\n",
    "    \n",
    "    # propagate uncertainties\n",
    "    for p in range(3):\n",
    "        lepton_to_hadron_ratios_u[e][p] = lepton_to_hadron_ratios[e][p] * \\\n",
    "            np.sqrt((cross_section_u[e][p]/cross_section[e][p])**2 + (cross_section_u[e][-1]/cross_section[e][-1])**2)\n",
    "\n",
    "\n",
    "## plot results\n",
    "for p, particle in enumerate(['$\\\\sigma^\\\\mathrm{e}/\\\\sigma^\\\\mathrm{q}$', '$\\\\sigma^\\\\mu/\\\\sigma^\\\\mathrm{q}$', '$\\\\sigma^\\\\tau/\\\\sigma^\\\\mathrm{q}$']):\n",
    "    plt.errorbar(2*LEP_energies, lepton_to_hadron_ratios[:,p], yerr=lepton_to_hadron_ratios_u[:,p], fmt=\"x\", label=particle)\n",
    "\n",
    "#plt.ylim(0.03, 0.06)\n",
    "plt.axhline(0.052716, label=\"theory\", c=\"r\")\n",
    "plt.legend(loc=8)\n",
    "plt.xlabel(\"center-of-mass energy $\\\\sqrt{s}$ [$\\\\mathrm{GeV}$]\")\n",
    "plt.ylabel(\"ratio of cross sections\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate number of light neutrinos\n",
    "\n",
    "# calculate invisible decay width\n",
    "inv_decay_width = z0_decay_width*1e3 - partial_ee \\\n",
    "                    - unit_factor2 * unit_factor3 * mm_popt[0]**2 * mm_popt[1]**2 * mm_popt[2] / (12 * np.pi * partial_ee) \\\n",
    "                    - unit_factor2 * unit_factor3 * tt_popt[0]**2 * tt_popt[1]**2 * tt_popt[2] / (12 * np.pi * partial_ee) \\\n",
    "                    - unit_factor2 * unit_factor3 * qq_popt[0]**2 * qq_popt[1]**2 * qq_popt[2] / (12 * np.pi * partial_ee)\n",
    "# propagate uncertainties and covariances\n",
    "# calculate partial derivatives\n",
    "ddPartial_ee = -1 + unit_factor2 * unit_factor3 * mm_popt[0]**2 * mm_popt[1]**2 * mm_popt[2] / (12 * np.pi * partial_ee**2) \\\n",
    "                  + unit_factor2 * unit_factor3 * tt_popt[0]**2 * tt_popt[1]**2 * tt_popt[2] / (12 * np.pi * partial_ee**2) \\\n",
    "                  + unit_factor2 * unit_factor3 * qq_popt[0]**2 * qq_popt[1]**2 * qq_popt[2] / (12 * np.pi * partial_ee**2)\n",
    "ddPeak_m = mm_popt[0]**2 * mm_popt[1]**2 / (12*np.pi * partial_ee)\n",
    "ddMass_m = mm_popt[0] * mm_popt[1]**2 * mm_popt[2] / (6*np.pi * partial_ee)\n",
    "ddGamma_m = mm_popt[0]**2 * mm_popt[1] * mm_popt[2] / (6*np.pi * partial_ee)\n",
    "ddPeak_t = tt_popt[0]**2 * tt_popt[1]**2 / (12*np.pi * partial_ee)\n",
    "ddMass_t = tt_popt[0] * tt_popt[1]**2 * tt_popt[2] / (6*np.pi * partial_ee)\n",
    "ddGamma_t = tt_popt[0]**2 * tt_popt[1] * tt_popt[2] / (6*np.pi * partial_ee)\n",
    "ddPeak_q = qq_popt[0]**2 * qq_popt[1]**2 / (12*np.pi * partial_ee)\n",
    "ddMass_q = qq_popt[0] * qq_popt[1]**2 * qq_popt[2] / (6*np.pi * partial_ee)\n",
    "ddGamma_q = qq_popt[0]**2 * qq_popt[1] * qq_popt[2] / (6*np.pi * partial_ee)\n",
    "# sum all uncertainties\n",
    "inv_decay_width_u = np.sqrt((z0_decay_width_u*1e3)**2 + (ddPartial_ee * partial_ee_u)**2 + \\\n",
    "                            ddPeak_m**2 * mm_pcov[2,2] + ddMass_m**2 * mm_pcov[0,0] + ddGamma_m**2 * mm_pcov[1,1] + \\\n",
    "                            ddPeak_t**2 * tt_pcov[2,2] + ddMass_t**2 * tt_pcov[0,0] + ddGamma_t**2 * tt_pcov[1,1] + \\\n",
    "                            ddPeak_q**2 * qq_pcov[2,2] + ddMass_q**2 * qq_pcov[0,0] + ddGamma_q**2 * qq_pcov[1,1])\n",
    "\n",
    "# divide by theoretical value per neutrino type\n",
    "number_neutrinos = inv_decay_width / 165.9\n",
    "number_neutrinos_u = inv_decay_width_u / 165.9\n",
    "\n",
    "print(\"Number of light neutrinos: \", number_neutrinos, \"+-\", number_neutrinos_u)\n",
    "print(\"Theory: 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
